---
title: "Lecture 4: Maximum likelihood estimation"
author: "Ciaran Evans"
output: beamer_presentation
---

## Recap: maximum likelihood estimation

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations, and let $f({\bf y}|\theta)$ denote the joint pdf or pmf of ${\bf Y}$, with parameter(s) $\theta$. The *likelihood function* is
$$L(\theta | {\bf Y}) = f({\bf Y} | \theta)$$

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations. The *maximum likelihood estimator* (MLE) is 

$$\widehat{\theta} = \ \text{argmax}_{\theta} \ L(\theta | {\bf Y})$$


## Example: $N(\theta, 1)$


## Example: $Uniform(0, \theta)$

Let $Y_1,...,Y_n \overset{iid}{\sim} Uniform(0, \theta)$, where $\theta > 0$. We want the maximum likelihood estimator of $\theta$.

Discuss with your neighbors what the MLE of $\theta$ might be. *Hint: focus on finding and sketching the likelihood function* $L({\bf Y}|\theta)$


## Example: $N(\mu, \sigma^2)$


## Linear regression with normal errors

$$Y_i \sim N(\mu_i, \sigma^2)$$

$$\mu_i = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_k X_{i,k}$$

Suppose we observe independent samples $(X_1, Y_1),...,(X_n, Y_n)$. Write down the likelihood function

$$L(\beta | {\bf X}, {\bf Y}) \propto \prod \limits_{i=1}^n f(Y_i| \beta, X_i)$$

for the linear regression problem.