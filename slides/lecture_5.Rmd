---
title: "Lecture 5: Maximum likelihood estimation"
author: "Ciaran Evans"
output: beamer_presentation
---

## Recap: maximum likelihood estimation

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations, and let $f({\bf y}|\theta)$ denote the joint pdf or pmf of ${\bf Y}$, with parameter(s) $\theta$. The *likelihood function* is
$$L(\theta | {\bf Y}) = f({\bf Y} | \theta)$$

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations. The *maximum likelihood estimator* (MLE) is 

$$\widehat{\theta} = \ \text{argmax}_{\theta} \ L(\theta | {\bf Y})$$


## Example: $N(\mu, \sigma^2)$


## Linear regression with normal errors

$$Y_i \sim N(\mu_i, \sigma^2)$$

$$\mu_i = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_k X_{i,k}$$

Suppose we observe independent samples $(X_1, Y_1),...,(X_n, Y_n)$. Write down the likelihood function

$$L(\beta | {\bf X}, {\bf Y}) \propto \prod \limits_{i=1}^n f(Y_i| \beta, X_i)$$

## Logistic regression

$$Y_i \sim Bernoulli(p_i)$$

$$\log \left( \frac{p_i}{1- p_i} \right) = \beta_0 + \beta_1 X_{i,1} + \cdots + \beta_k X_{i,k}$$
Suppose we observe independent samples $(X_1, Y_1),...,(X_n, Y_n)$. Write down the likelihood function

$$L(\beta | {\bf X}, {\bf Y}) \propto \prod \limits_{i=1}^n f(Y_i| \beta, X_i)$$



