---
title: "Lecture 30: Comparing estimators"
author: "Ciaran Evans"
output: beamer_presentation
---

## Course so far

* Maximum likelihood estimation
* Logistic regression
* Asymptotics
* Asymptotic properties of MLEs
* Hypothesis testing
* Confidence intervals

\bigskip

**Common theme:** Likelihoods and MLEs

\bigskip

**Question:** Why maximum likelihood estimation?

\bigskip

**Today:** 

* Another approach to estimation: method of moments
* What makes a good estimator?

## Example

Suppose $X_1,...,X_n \overset{iid}{\sim} Uniform(0, \theta)$. How could I estimate $\theta$?

\vspace{6cm}

## Example

Suppose $X_1,...,X_n \overset{iid}{\sim} Uniform(a, b)$. How could I estimate $a$ and $b$?

\vspace{6cm}

## Method of moments

Let $X_1,...,X_n$ be a sample from a distribution with probability function $f(x|\theta_1,...,\theta_k)$, with $k$ parameters $\theta_1,...,\theta_k$.

\vspace{6cm}

## Example

Suppose $X_1,...,X_n \overset{iid}{\sim} N(\mu, \sigma^2)$.

\vspace{6cm}

## What makes a good estimator?

Suppose $X_1,...,X_n \overset{iid}{\sim} Uniform(0, \theta)$. Two possible estimates:

$$\text{MLE: }\ \widehat{\theta} = X_{(n)} \hspace{1cm} \text{MoM: } \ \widehat{\theta} = 2 \overline{X}$$

**Question:** How would I choose between these estimators? 

\vspace{5cm}

## Bias, variance, and MSE

## Example

Suppose $X_1,...,X_n \overset{iid}{\sim} Uniform(0, \theta)$.

\vspace{6cm}


## Example

Suppose $X_1,...,X_n \overset{iid}{\sim} N(\mu, \sigma^2)$. Consider two estimates of $\sigma^2$:
$$\widehat{\sigma}^2 = \frac{1}{n} \sum \limits_{i=1}^n (X_i - \overline{X})^2 \hspace{1cm} s^2 = \frac{1}{n-1} \sum \limits_{i=1}^n (X_i - \overline{X})^2$$

\vspace{5cm}







