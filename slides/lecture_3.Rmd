---
title: "Lecture 3: Maximum likelihood estimation"
author: "Ciaran Evans"
output: beamer_presentation
---


## Motivation: fitting a *linear* regression model

$$Y_i = \beta_0 + \beta_1 X_{i, 1} + \beta_2 X_{i, 2} + \cdots + \beta_k X_{i, k} + \varepsilon_i \hspace{1.5cm} \varepsilon_i \overset{iid}{\sim} N(0, \sigma_\varepsilon^2)$$

Suppose we observe data $(X_1, Y_1), (X_2, Y_2), ..., (X_n, Y_n)$, where $X_i = (1, X_{i,1}, ..., X_{i,k})^T$.

How do we fit this linear regression model? That is, how do we estimate $$\beta = (\beta_0, \beta_1, ..., \beta_k)^T$$

\vspace{4.5cm}


## Fitting a *logistic* regression model?

Linear regression: minimize $\sum \limits_{i=1}^n (Y_i - \beta_0 - \beta_1 X_{i,1} - \cdots - \beta_k X_{i,k})^2$

\bigskip

**Question:** Should we minimize a similar sum of squares for a *logistic* regression model?

\vspace{4cm}


## Motivation: likelihoods and estimation

Let $Y \sim Bernoulli(p)$ be a Bernoulli random variable, with $p \in [0,1]$. We observe 5 independent samples from this distribution:

$$Y_1 = 1, \ Y_2 = 1, \ Y_3 = 0, \ Y_4 = 0, \ Y_5 = 1$$

The true value of $p$ is unknown, so two friends propose different guesses for the value of $p$: 0.3 and 0.7. Which do you think is a "better" guess?

\vspace{3cm}


## Likelihood

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations, and let $f({\bf y}|\theta)$ denote the joint pdf or pmf of ${\bf Y}$, with parameter(s) $\theta$. The *likelihood function* is
$$L(\theta | {\bf Y}) = f({\bf Y} | \theta)$$

\vspace{5cm}


## Example: Bernoulli data


## Example: Bernoulli data

$Y_1,...,Y_5 \overset{iid}{\sim} Bernoulli(p)$, with observed data
$$Y_1 = 1, \ Y_2 = 1, \ Y_3 = 0, \ Y_4 = 0, \ Y_5 = 1$$

$L(p|{\bf Y}) = p^3(1 - p)^2$

```{r, echo=F, fig.align='center', fig.width=3, fig.height=2.5, message=F, warning=F}
library(tidyverse)
p <- seq(0, 1, 0.01)
y <- p^3*(1 - p)^2
data.frame(p, y) %>%
  ggplot(aes(x = p, y = y)) +
  geom_line() +
  theme_bw() +
  labs(x = "p", y = "L(p|Y)")
```

## Maximum likelihood estimator

**Definition:** Let ${\bf Y} = (Y_1,...,Y_n)$ be a sample of $n$ observations. The *maximum likelihood estimator* (MLE) is 

$$\widehat{\theta} = \ \text{argmax}_{\theta} \ L(\theta | {\bf Y})$$

\vspace{4cm}


## Example: $Bernoulli(p)$
